"""
Functions for processing individual CS2 demo files.
"""

import os
import sys
import json
import logging
import asyncio
import re
from typing import Dict, List, Counter

from commands.parser.utils import format_match_id, extract_short_id

logger = logging.getLogger('discord_bot')
debug_logger = logging.getLogger('debug_discord_bot')

# Path to the parser scripts
PARSER_INTERFACE = r"C:\demofetch\CSharpParser\demoparser-main\examples\demoparse_full\discord_interface.py"

def count_tickbytick_by_type(output_files: List[str]) -> Dict[str, int]:
    """
    Count the number of tick-by-tick files by type (ACE, QUAD, TRIPLE, etc.)
    
    Args:
        output_files: List of output file paths
        
    Returns:
        Dict[str, int]: Count of files by type
    """
    counts = Counter()
    
    for file_path in output_files:
        # Extract the type from the filename (e.g., ACE_mirage_1-fc7d5cea-8011-40ed-849b-0079bfb59cad_3_5.csv)
        filename = os.path.basename(file_path)
        match = re.match(r'^([A-Z]+)_', filename)
        if match:
            collection_type = match.group(1)
            counts[collection_type] += 1
    
    return counts

async def process_demo(demo_id: str, demo_path: str, month: str, parser_stats: Dict, stop_event: asyncio.Event) -> bool:
    """
    Process a single demo using the parser
    
    Args:
        demo_id: Demo ID
        demo_path: Path to the demo file
        month: Month name
        parser_stats: Dictionary to track parser statistics
        stop_event: Event to signal stopping
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        short_id = extract_short_id(demo_id)
        debug_logger.info(f"Processing demo: {demo_id}")
        
        # Step 1: Process kill collection
        debug_logger.info("Running kill collection parser...")
        kill_collection_cmd = [
            sys.executable, 
            PARSER_INTERFACE, 
            "killcollection", 
            demo_path
        ]
        
        kill_collection_process = await asyncio.create_subprocess_exec(
            *kill_collection_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await kill_collection_process.communicate()
        
        if kill_collection_process.returncode != 0:
            logger.error(f"Kill collection parser failed with code {kill_collection_process.returncode}")
            logger.error(f"Stderr: {stderr.decode()}")
            print(f"[✗] Failed demo: {format_match_id(demo_id)} - Parser error")
            return False
        
        # Parse the JSON output to get the generated CSV files
        try:
            # Extract the JSON part from the output
            stdout_text = stdout.decode()
            
            # Find the JSON part (should be the last part of the output)
            json_start = stdout_text.rfind('{')
            if json_start == -1:
                logger.error("No JSON found in kill collection parser output")
                print(f"[✗] Failed demo: {format_match_id(demo_id)} - No JSON in output")
                return False
                
            json_text = stdout_text[json_start:]
            result = json.loads(json_text)
            
            if not result.get('success', False):
                error_msg = result.get('message', 'Unknown error')
                logger.error(f"Kill collection parser reported failure: {error_msg}")
                print(f"[✗] Failed demo: {format_match_id(demo_id)} - {error_msg}")
                return False
            
            # Get the output CSV files
            output_files = result.get('output_files', [])
            if not output_files:
                logger.warning("No output files generated by kill collection parser")
                print(f"[✗] Failed demo: {format_match_id(demo_id)} - No collections generated")
                return False
            
            # Update stats
            parser_stats['kill_collections'] += len(output_files)
            
            # Log success for kill collections
            print(f"[✓] Kill Collections wrote to master files ({short_id})")
            
            # Step 2: Process tick-by-tick for each output file
            debug_logger.info(f"Running tick-by-tick parser on {len(output_files)} files...")
            
            all_tickbytick_files = []
            
            for csv_file in output_files:
                if stop_event.is_set():
                    logger.info("Parser stopped, aborting processing")
                    return False
                
                debug_logger.info(f"Processing tick-by-tick for: {csv_file}")
                
                # Get config
                from commands.parser.config import get_config
                config = get_config()
                
                # Create the TickByTick directory structure if it doesn't exist
                tick_base_dir = config.get('project', {}).get('TickByTickParse', '')
                if tick_base_dir:
                    # Get month from the CSV file
                    month_dir = os.path.basename(os.path.dirname(csv_file))
                    tick_month_dir = os.path.join(tick_base_dir, month_dir)
                    os.makedirs(tick_month_dir, exist_ok=True)
                    
                    # Also create subdirectories for different collection types
                    for collection_type in ['ACE', 'QUAD', 'TRIPLE']:
                        type_dir = os.path.join(tick_month_dir, collection_type)
                        os.makedirs(type_dir, exist_ok=True)
                
                tick_cmd = [
                    sys.executable, 
                    PARSER_INTERFACE, 
                    "tickbytick", 
                    csv_file
                ]
                
                tick_process = await asyncio.create_subprocess_exec(
                    *tick_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                
                tick_stdout, tick_stderr = await tick_process.communicate()
                
                if tick_process.returncode != 0:
                    logger.error(f"Tick-by-tick parser failed with code {tick_process.returncode}")
                    logger.error(f"Stderr: {tick_stderr.decode()}")
                    # Continue with other files even if one fails
                    continue
                
                try:
                    # Extract the JSON part from the output
                    tick_stdout_text = tick_stdout.decode()
                    
                    # Find the JSON part (should be the last part of the output)
                    json_start = tick_stdout_text.rfind('{')
                    if json_start == -1:
                        logger.error("No JSON found in tick-by-tick parser output")
                        # Continue with other files even if one fails
                        continue
                        
                    json_text = tick_stdout_text[json_start:]
                    tick_result = json.loads(json_text)
                    
                    if not tick_result.get('success', False):
                        logger.warning(f"Tick-by-tick parser reported failure: {tick_result.get('message', 'Unknown error')}")
                        # Continue with other files even if one fails
                        continue
                    
                    # Get the output files
                    tick_output_files = tick_result.get('output_files', [])
                    all_tickbytick_files.extend(tick_output_files)
                    
                    debug_logger.info(f"Successfully processed tick-by-tick for: {csv_file}")
                except json.JSONDecodeError:
                    logger.error(f"Failed to parse tick-by-tick output: {tick_stdout_text}")
                    # Continue with other files even if one fails
                    continue
                except Exception as e:
                    logger.error(f"Error processing tick-by-tick for {csv_file}: {str(e)}")
                    # Continue with other files even if one fails
                    continue
            
            # Count tick-by-tick files by type
            type_counts = count_tickbytick_by_type(all_tickbytick_files)
            
            # Update stats
            parser_stats['tickbytick_files'] += len(all_tickbytick_files)
            
            # Log success for tick-by-tick with counts by type
            type_summary = " ".join([f"{count} {type}S" for type, count in sorted(type_counts.items())])
            if not type_summary:
                type_summary = "0 collections"
            
            print(f"[✓] Tick Parsed: {type_summary} ({short_id})")
            
            # If we got here, the demo was processed successfully
            logger.info(f"Successfully processed demo: {demo_id}")
            return True
            
        except json.JSONDecodeError:
            logger.error(f"Failed to parse kill collection output: {stdout.decode()}")
            print(f"[✗] Failed demo: {format_match_id(demo_id)} - Invalid parser output")
            return False
        
    except Exception as e:
        logger.error(f"Error processing demo {demo_id}: {str(e)}")
        print(f"[✗] Failed demo: {format_match_id(demo_id)} - {str(e)}")
        return False
